{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq llama-index-llms-groq\n",
    "%pip install -Uq llama-index\n",
    "%pip install -Uq python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm v0 \n",
    "Multiple chat com resisor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "SYSTEM_PROMPT = open('prompts/system_00.md', 'r', encoding='utf-8').read()\n",
    "\n",
    "APPEND_PROMPT = \"RESPIRA FUNDO, REVÊ CUIDADOSAMENTE O CONTEÚDO ORIGINAL, E AGORA COMPLETA A VERSÃO FINAL DO CONTEÚDO FORMATADO. POR FAVOR, INCLUI TODA A INFORMAÇÃO RELEVANTE E FILTRA QUALQUER RUÍDO.\"\n",
    "\n",
    "REVISOR_PROMPT = open('prompts/revisor_prompt_00.md', 'r', encoding='utf-8').read()\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=GROQ_API_KEY, temperature=0.1)\n",
    "\n",
    "    def complete(self, prompt, context=None, system_prompt=SYSTEM_PROMPT):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        if context is not None:\n",
    "            messages.append(ChatMessage(role=\"user\", content=f\"Context: {context}\"))\n",
    "        \n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "    def chat(self, messages):\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Step 1: Initial processing\n",
    "    initial_prompt = f\"{content}\\n\\n{APPEND_PROMPT}\"\n",
    "    initial_response = llm.complete(initial_prompt)\n",
    "\n",
    "    # Step 2: Revision\n",
    "    revision_messages = [\n",
    "        ChatMessage(role=\"system\", content=SYSTEM_PROMPT),\n",
    "        ChatMessage(role=\"user\", content=initial_prompt),\n",
    "        ChatMessage(role=\"assistant\", content=initial_response),\n",
    "        ChatMessage(role=\"user\", content=REVISOR_PROMPT),\n",
    "    ]\n",
    "    final_response = llm.chat(revision_messages)\n",
    "\n",
    "    return final_response #.message.content\n",
    "\n",
    "def main():\n",
    "    llm = GroqLLM()\n",
    "    input_folder = 'output2'\n",
    "    output_folder = 'output_clean_formatted'\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.md'):  # Assuming the files are markdown\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            print(f\"Processing {filename}...\")\n",
    "            processed_content = process_file(input_path, llm)\n",
    "\n",
    "            # Save the processed content\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_content)\n",
    "\n",
    "            print(f\"Saved processed content to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm v3\n",
    "\n",
    "Apenas um system e passa os ficheiro para a llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='processing_log.txt', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "SYSTEM_PROMPT = open('prompts/system_01.md', 'r', encoding='utf-8').read()\n",
    "\n",
    "#\"llama-guard-3-8b\"\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            {\"name\": \"llama-3.1-70b-versatile\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.1-8b-instant\", \"context_window\": 8192},\n",
    "            {\"name\": \"mixtral-8x7b-32768\", \"context_window\": 32768},\n",
    "            {\"name\": \"llama3-70b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-8b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma2-9b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma-7b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-90b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-11b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-70b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-8b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-3b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-1b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-guard-3-8b\", \"context_window\": 8192},\n",
    "            {\"name\": \"llava-v1.5-7b-4096-preview\", \"context_window\": 4096},\n",
    "        ]\n",
    "        self.current_model_index = 0\n",
    "        self.llm = None\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "        self.initialize_llm()\n",
    "\n",
    "    def initialize_llm(self):\n",
    "        self.llm = Groq(model=self.models[self.current_model_index][\"name\"], api_key=GROQ_API_KEY, temperature=0.1)\n",
    "\n",
    "    def rotate_model(self):\n",
    "        self.current_model_index = (self.current_model_index + 1) % len(self.models)\n",
    "        self.initialize_llm()\n",
    "        logging.info(f\"Switched to model: {self.get_current_model()}\")\n",
    "\n",
    "    def respect_rate_limit(self):\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_request_time < 60:  # 60 seconds window\n",
    "            if self.request_count >= 14400 / 1440:  # 14400 requests per day, divided by minutes in a day\n",
    "                sleep_time = 60 - (current_time - self.last_request_time)\n",
    "                logging.info(f\"Rate limit approached. Sleeping for {sleep_time:.2f} seconds\")\n",
    "                time.sleep(sleep_time)\n",
    "                self.request_count = 0\n",
    "                self.last_request_time = time.time()\n",
    "        else:\n",
    "            self.request_count = 0\n",
    "            self.last_request_time = current_time\n",
    "        self.request_count += 1\n",
    "\n",
    "    def chat(self, messages, max_retries=3, retry_delay=5):\n",
    "        original_model_index = self.current_model_index\n",
    "        for attempt in range(max_retries * len(self.models)):\n",
    "            try:\n",
    "                self.respect_rate_limit()\n",
    "                response = self.llm.chat(messages)\n",
    "                logging.info(f\"Successfully processed using model: {self.get_current_model()}\")\n",
    "                return response.message.content\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                logging.warning(f\"Error with model {self.get_current_model()}: {error_message}. Attempt {attempt + 1} of {max_retries * len(self.models)}\")\n",
    "                \n",
    "                if \"429\" in error_message:\n",
    "                    # Rate limit error\n",
    "                    retry_after = int(error_message.split(\"Please try again in \")[1].split(\"s.\")[0])\n",
    "                    logging.info(f\"Rate limit exceeded. Waiting for {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                elif \"400\" in error_message and \"maximum context length\" in error_message.lower():\n",
    "                    # Context length error\n",
    "                    logging.info(f\"Context length exceeded for model {self.get_current_model()}. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                elif \"404\" in error_message and \"model does not exist\" in error_message.lower():\n",
    "                    # Model not found error\n",
    "                    logging.info(f\"Model {self.get_current_model()} not found. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                else:\n",
    "                    # Other errors\n",
    "                    if attempt % max_retries == max_retries - 1:\n",
    "                        logging.info(f\"Rotating to next model after multiple failures.\")\n",
    "                        self.rotate_model()\n",
    "                    else:\n",
    "                        logging.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                        time.sleep(retry_delay)\n",
    "                \n",
    "                # If we've tried all models and are back to the original, break the loop\n",
    "                if self.current_model_index == original_model_index and attempt >= len(self.models):\n",
    "                    break\n",
    "\n",
    "        logging.error(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "        raise Exception(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_model_index][\"name\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=SYSTEM_PROMPT),\n",
    "        ChatMessage(role=\"user\", content=content)\n",
    "    ]\n",
    "\n",
    "    current_model = llm.get_current_model()\n",
    "    logging.info(f\"Processing file {file_path} with model: {current_model}\")\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    \n",
    "    logging.info(f\"Finished processing {file_path} with model: {current_model}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        llm = GroqLLM()\n",
    "        input_folder = 'out'\n",
    "        output_folder = 'output_clean_formatted'\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        files_to_process = [f for f in os.listdir(input_folder) if f.endswith('.md')]\n",
    "        logging.info(f\"Found {len(files_to_process)} .md files in the input folder.\")\n",
    "\n",
    "        for filename in files_to_process:\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                logging.info(f\"Skipping {filename} as it already exists in the output folder.\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Starting to process {filename}...\")\n",
    "            try:\n",
    "                processed_content = process_file(input_path, llm)\n",
    "\n",
    "                with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(processed_content)\n",
    "\n",
    "                logging.info(f\"Saved processed content to {output_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filename}: {str(e)}\")\n",
    "                logging.exception(\"Exception details:\")\n",
    "\n",
    "        logging.info(\"Finished processing all files.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to optimize llm v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            {\"name\": \"llama-3.1-70b-versatile\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.1-8b-instant\", \"context_window\": 8192},\n",
    "            {\"name\": \"mixtral-8x7b-32768\", \"context_window\": 32768},\n",
    "            {\"name\": \"llama3-70b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-8b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma2-9b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma-7b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-90b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-11b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-70b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-8b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-3b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-1b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-guard-3-8b\", \"context_window\": 8192},\n",
    "            {\"name\": \"llava-v1.5-7b-4096-preview\", \"context_window\": 4096},\n",
    "        ]\n",
    "        self.current_model_index = 0\n",
    "        self.last_request_time = 0\n",
    "        self.requests_in_window = 0\n",
    "        self.rate_limit = 500000  # tokens per minute\n",
    "        self.window_size = 60  # 1 minute window\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_model_index]\n",
    "\n",
    "    def rotate_model(self):\n",
    "        self.current_model_index = (self.current_model_index + 1) % len(self.models)\n",
    "        logging.info(f\"Rotating to model: {self.get_current_model()['name']}\")\n",
    "\n",
    "    def wait_for_rate_limit(self, tokens_requested):\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "\n",
    "        if time_since_last_request >= self.window_size:\n",
    "            self.requests_in_window = 0\n",
    "        else:\n",
    "            self.requests_in_window -= max(0, (self.requests_in_window * time_since_last_request / self.window_size))\n",
    "\n",
    "        if self.requests_in_window + tokens_requested > self.rate_limit:\n",
    "            wait_time = (self.requests_in_window + tokens_requested - self.rate_limit) / (self.rate_limit / self.window_size)\n",
    "            logging.info(f\"Rate limit approached. Waiting for {wait_time:.2f} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        self.requests_in_window += tokens_requested\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def chat(self, messages, max_retries=3):\n",
    "        for attempt in range(max_retries * len(self.models)):\n",
    "            current_model = self.get_current_model()\n",
    "            llm = Groq(model=current_model[\"name\"])\n",
    "\n",
    "            try:\n",
    "                # Estimate token count (this is a rough estimate, you may want to use a proper tokenizer)\n",
    "                tokens_requested = sum(len(msg.content.split()) for msg in messages) * 1.3\n",
    "\n",
    "                self.wait_for_rate_limit(tokens_requested)\n",
    "\n",
    "                response = llm.chat(messages)\n",
    "                return response.message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                logging.warning(f\"Error with model {current_model['name']}: {error_message}. Attempt {attempt + 1} of {max_retries * len(self.models)}\")\n",
    "\n",
    "                if \"429\" in error_message:\n",
    "                    retry_after = 60  # Default to 60 seconds if we can't parse the time\n",
    "                    try:\n",
    "                        retry_after = int(float(error_message.split(\"Please try again in \")[1].split(\"s\")[0]))\n",
    "                    except:\n",
    "                        pass\n",
    "                    logging.info(f\"Rate limit exceeded. Waiting for {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                    self.rotate_model()\n",
    "                elif attempt % max_retries == max_retries - 1:\n",
    "                    self.rotate_model()\n",
    "                else:\n",
    "                    time.sleep(5)  # Wait for 5 seconds before retrying with the same model\n",
    "\n",
    "        logging.error(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are a helpful assistant that cleans and formats text.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"Please clean and format the following text, removing any HTML tags, fixing formatting issues, and ensuring it's well-structured:\\n\\n{content}\")\n",
    "        ]\n",
    "\n",
    "        processed_content = llm.chat(messages)\n",
    "        return processed_content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {file_path}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def main():\n",
    "    llm = GroqLLM()\n",
    "    input_folder = 'out'\n",
    "    output_folder = 'output_clean_formatted'\n",
    "    failed_folder = 'failed_processing'\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(failed_folder, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(input_folder) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files_to_process)} .md files in the input folder.\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            logging.info(f\"Skipping {filename} as it already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Starting to process {filename}...\")\n",
    "        processed_content = process_file(input_path, llm)\n",
    "\n",
    "        if processed_content:\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_content)\n",
    "            logging.info(f\"Saved processed content to {output_path}\")\n",
    "        else:\n",
    "            failed_path = os.path.join(failed_folder, filename)\n",
    "            os.rename(input_path, failed_path)\n",
    "            logging.warning(f\"Moved {filename} to {failed_folder} due to processing failure.\")\n",
    "\n",
    "    logging.info(\"Finished processing all files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
