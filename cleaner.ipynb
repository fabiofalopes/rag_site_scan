{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq llama-index-llms-groq\n",
    "%pip install -Uq llama-index\n",
    "%pip install -Uq python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "SYSTEM_PROMPT = '''\n",
    "\n",
    "**Objective**:\n",
    "Transform web-scraped markdown documents into a clear, comprehensive representation of essential content. The focus is on extracting all valuable information, ensuring that the main content of the page is fully captured and presented. While filtering out repetitive and irrelevant text, ensure that the content length is preserved and formatted into a well-structured markdown format. Ensure the output reflects the true content, context, and structure of the original webpage, maintaining the same language as the input document.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions for the System**:\n",
    "\n",
    "1. **Initial Header**:  \n",
    "   - Begin each transformed output with: `\"# Response for <url>\"`, inserting the URL found in the file. This acts as a consistent identifier for the source of the content.\n",
    "\n",
    "2. **Complete Core Content Identification and Extraction**:  \n",
    "   - Ensure that all main content from the raw markdown file is thoroughly examined and extracted, maintaining the full content length.\n",
    "   - This includes detailed articles, main text bodies, descriptions, and any substantial information.\n",
    "   - Guarantee that all content of informational value is included; nothing should be omitted that contributes to the page's main message and context.\n",
    "\n",
    "3. **Link Evaluation and Inclusion**:\n",
    "   - **Preserve** links only if they provide significant relevance, additional details, or direct relevance to the core content.\n",
    "   - **Exclude** links commonly found in navigation menus, headers, footers, or site-wide elements like terms of service and privacy policy unless directly relevant to the page content.\n",
    "\n",
    "4. **Information Structure and Formatting**:\n",
    "   - Employ markdown syntax to organize the output effectively, using headings, subheadings, lists, and bullet points to replicate the hierarchical and categorical structure of the original content.\n",
    "   - Ensure that content flows logically and contextually, maintaining the essence and message of the source material.\n",
    "   - Highlight and retain any contextually important links using markdown `[link text](URL)` format.\n",
    "\n",
    "5. **Content Integrity and Verbosity**:\n",
    "   - Keep the extracted content faithful to its original form without paraphrasing, ensuring a detailed representation that captures all essential information and nuances.\n",
    "   - Deliver a verbose output that preserves the full length of the original content, presenting an accurate and immediately usable document.\n",
    "\n",
    "6. **Quality and Consistency**:\n",
    "   - Ensure precision and uniformity across outputs for different input files, respecting the specificity and uniqueness of each webpage’s content.\n",
    "   - Pay strict attention to the details that characterize and define the content’s context and purpose.\n",
    "\n",
    "7. **Language Consistency**:\n",
    "   - Always produce the output in the same language as the raw input document to maintain linguistic accuracy and coherence.\n",
    "\n",
    "---\n",
    "\n",
    "**Content Processing Instructions**:\n",
    "\n",
    "- **Pattern-based Noise Removal**:\n",
    "  - Remove elements that are typically navigational or promotional, such as:\n",
    "    - Menu items (e.g., \"Cursos\", \"Notícias\", \"Investigação\", \"Eventos\", \"Candidaturas\").\n",
    "    - Links unrelated to the main topic like \"Página Inicial\", \"Office 365\", or \"Moodle\".\n",
    "    - Social media links (e.g., \"Facebook\", \"Twitter\", \"Instagram\").\n",
    "    - Administrative or utility links and text items (e.g., \"Login\", \"Subscribe\", \"Newsletter\").\n",
    "    - Repeated elements like footers and standard disclaimers.\n",
    "\n",
    "- **Focus Extraction**:\n",
    "  - Concentrate on sections that exhibit clearly defined information, such as structured headings in the raw markdown that correlate with substantive text.\n",
    "\n",
    "- **Link Relevance Check**:\n",
    "  - Retain links that significantly enhance the content, disregarding generic site links unless directly tied to the primary content focus.\n",
    "\n",
    "- **Content Authenticity**:\n",
    "  - Preserve the authenticity and factual representation of the webpage’s information, ensuring it aligns with the original page's context and message.\n",
    "\n",
    "---\n",
    "\n",
    "**Reminder**: This prompt is a guideline designed to cover a wide variety of content while consistently achieving the objective of extracting and structuring meaningful information. Always adapt according to the unique characteristics presented within each file, ensuring completeness, full capture of the main content with its length preserved, and maintaining the original language.\n",
    "\n",
    "'''\n",
    "\n",
    "APPEND_PROMPT = \"RESPIRA FUNDO, REVÊ CUIDADOSAMENTE O CONTEÚDO ORIGINAL, E AGORA COMPLETA A VERSÃO FINAL DO CONTEÚDO FORMATADO. POR FAVOR, INCLUI TODA A INFORMAÇÃO RELEVANTE E FILTRA QUALQUER RUÍDO.\"\n",
    "\n",
    "REVISOR_PROMPT_bk_0 = '''\n",
    "\n",
    "Por favor, revê atentamente o teu texto formatado tendo em conta as seguintes instruções. Analisa o resultado anterior gerado pelo modelo juntamente com o prompt do sistema e o conteúdo bruto original. O objetivo é assegurar que o documento final formatado atende a todos os requisitos do sistema, capturando toda a informação relevante e mantendo a integridade do conteúdo.\n",
    "\n",
    "1. **Compreensão Completa**: Verifica se todo o conteúdo principal do documento original foi capturado. Assegura que nenhuma informação relevante foi omitida e que o texto reflete fielmente o conteúdo original.\n",
    "\n",
    "2. **Estrutura e Formatação**: Confirma que o texto está devidamente estruturado em markdown, utilizando cabeçalhos, subtítulos e listas para organizar a informação de forma lógica e categórica.\n",
    "\n",
    "3. **Coerência de Língua**: Garante que o output é apresentado na mesma língua que o documento original, preservando a coerência linguística.\n",
    "\n",
    "4. **Links e Contexto**: Certifica-te de que todos os links importantes e contextualmente relevantes foram incluídos, enquanto os links de navegação e não relevantes foram eliminados.\n",
    "\n",
    "5. **Preservação do Conteúdo**: Assegura-te de que o comprimento do conteúdo não foi reduzido, mantendo toda a informação essencial e nuances do texto original.\n",
    "\n",
    "Realiza as correções, adições ou ajustes necessários para alinhar o documento final com os requisitos do sistema. Quando terminares, o output deverá ser uma representação fiel, completa e bem estruturada do conteúdo original, pronto para utilização imediata sem necessidade de edição adicional.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "REVISOR_PROMPT = '''\n",
    "\n",
    "Por favor, revê atentamente o teu texto formatado tendo em conta as seguintes instruções. Analisa o resultado anterior gerado pelo modelo juntamente com o SYSTEM PROMPT e o conteúdo bruto original. O objetivo é assegurar que o documento final formatado atende a todos os requisitos do sistema, capturando toda a informação relevante e mantendo a integridade do conteúdo.\n",
    "\n",
    "1. **Compreensão Completa**: Verifica se todo o conteúdo principal do documento original foi capturado. Assegura que nenhuma informação relevante foi omitida e que o texto reflete fielmente o conteúdo original.\n",
    "\n",
    "2. **Estrutura e Formatação**: Confirma que o texto está devidamente estruturado em markdown, utilizando cabeçalhos, subtítulos e listas para organizar a informação de forma lógica e categórica.\n",
    "\n",
    "3. **Coerência de Língua**: Garante que o output é apresentado na mesma língua que o documento original, preservando a coerência linguística.\n",
    "\n",
    "4. **Links e Contexto**: Certifica-te de que todos os links importantes e contextualmente relevantes foram incluídos, enquanto os links de navegação e não relevantes foram eliminados.\n",
    "\n",
    "5. **Preservação do Conteúdo**: Assegura-te de que o comprimento do conteúdo não foi reduzido, mantendo toda a informação essencial e nuances do texto original.\n",
    "\n",
    "Realiza as correções, adições ou ajustes necessários para alinhar o documento final com os requisitos do sistema. Quando terminares, o output deverá ser uma representação fiel, completa e bem estruturada do conteúdo original, pronto para utilização imediata sem necessidade de edição adicional.\n",
    "\n",
    "**Nota super importante**: Ao responder, o modelo nunca deve incluir frases que são típicas de textos gerados por LLM que visam soar formais e estruturados, mas que muitas vezes parecem genéricas ou desnecessárias. Para evitar isso, é essencial usar uma linguagem direta e concisa, eliminando palavras ou frases de preenchimento que não acrescentam valor. O modelo deve priorizar a clareza e a concisão, apresentando apenas a informação essencial ou o resultado específico de forma direta. \n",
    "\n",
    "### O que descartar:\n",
    "\n",
    "Evita incluir exemplos como a listagem abaixo, que contém informações de navegação, links de serviços e outras referências irrelevantes para o propósito da análise:\n",
    "\n",
    "```markdown\n",
    "## Universidade Lusófona\n",
    "### Apresentação\n",
    "\n",
    "A Universidade Lusófona é uma instituição de ensino superior que oferece uma variedade de cursos e programas acadêmicos.\n",
    "\n",
    "### Serviços\n",
    "\n",
    "- **Contacts**: https://www.ulusofona.pt/en/contacts\n",
    "- **Password Change and Recovery**: https://secure.ensinolusofona.pt/alteracao_password/f?p=133:2\n",
    "- **Help us to improve**: https://ulusofona.typeform.com/to/cipp2UFI\n",
    "- **Lost and Found**: https://www.ulusofona.pt/en/lost-and-found\n",
    "\n",
    "### Cursos\n",
    "\n",
    "- **Bachelors**: https://www.ulusofona.pt/en/undergraduate\n",
    "- **Masters**: https://www.ulusofona.pt/en/masters\n",
    "- **PhD**: https://www.ulusofona.pt/en/phd\n",
    "- **Post-graduation**: https://www.ulusofona.pt/en/post-graduation\n",
    "- **All the courses**: https://www.ulusofona.pt/en/courses\n",
    "\n",
    "### Documentos\n",
    "\n",
    "- **Fees and Emoluments**: https://www.ulusofona.pt/en/documents?cat=5\n",
    "- **Regulations and Orders**: https://www.ulusofona.pt/en/documents?cat=1\n",
    "- **Forms**: https://www.ulusofona.pt/en/documents?cat=13\n",
    "- **Reports**: https://www.ulusofona.pt/en/documents?cat=4\n",
    "- **Validation of documents**: https://www.ulusofona.pt/en/validation-of-issued-documents\n",
    "\n",
    "### Informação Acadêmica\n",
    "\n",
    "- **Calendários Acadêmicos**: https://www.ulusofona.pt/en/calendars\n",
    "- **Faculdades e Escolas**: https://www.ulusofona.pt/en/faculties-and-schools\n",
    "- **Plano de Género e Diversidade**: https://www.ensinolusofona.pt/en/gender-and-diversity-plan\n",
    "- **Propinas**: https://www.ulusofona.pt/en/fees\n",
    "- **Motivos para Frequência**: https://razoes.ulusofona.pt/\n",
    "- **Qualidade**: https://www.ulusofona.pt/en/qualidade\n",
    "- **Sobre Nós**: https://www.ulusofona.pt/en/about\n",
    "- **Visite-nos**: https://ulusofona.typeform.com/to/ypj6qk\n",
    "- **Canal de Denúncias Internas**: https://www.ulusofona.pt/en/internal-reporting-channel\n",
    "\n",
    "### Infraestruturas\n",
    "\n",
    "- **Campus**: https://campus.ulusofona.pt/\n",
    "- **Contactos**: https://www.ulusofona.pt/en/contacts\n",
    "- **Entidade Promotora**: https://www.cofac.pt\n",
    "- **Lusófona no Mundo**: https://www.ensinolusofona.pt/en/\n",
    "- **Lusófona 360º**: https://vr360.ulusofona.pt/visitavirtual_EN/\n",
    "\n",
    "### Professores\n",
    "\n",
    "- **Avadoc**: https://www.ulusofona.pt/avadoc\n",
    "- **Bem-vindos**: https://boasvindas.ulusofona.pt/\n",
    "- **Carreira Docente**: https://www.ulusofona.pt/en/documents?q=Career\n",
    "- **Diretório**: https://diretorio.ulusofona.pt/\n",
    "- **Emprego Científico**: https://www.ulusofona.pt/en/open-positions/scientific-employment-and-research-grants\n",
    "- **Portal do Docente**: https://secure.ensinolusofona.pt/ficha_docente/f?p=123:LOGIN_DESKTOP::::::\n",
    "- **Portal do Colaborador**: https://colaborador.ensinolusofona.pt/mygiaf/Login.xhtml\n",
    "\n",
    "### Investigação\n",
    "\n",
    "- **Portal de Investigação**: https://research.ulusofona.pt/\n",
    "- **ReCiL - Repositório Científico**: https://recil.ensinolusofona.pt/\n",
    "- **Revistas Científicas**: https://revistas.ulusofona.pt/\n",
    "- **Unidades de Investigação**: https://investigacao.ulusofona.pt/\n",
    "\n",
    "### Recursos\n",
    "\n",
    "- **Biblioteca**: https://biblioteca.ulusofona.pt/\n",
    "- **Click - Portal de Aprendizagem**: https://www.ulusofona.pt/en/click\n",
    "- **Documentos**: https://www.ulusofona.pt/documents\n",
    "- **FAQ - Centro de Ajuda**: https://www.ulusofona.pt/en/faqs\n",
    "- **Guia de Bem-vindo**: https://bemvindo.ulusofona.pt/\n",
    "- **Logótipos e Identidade Gráfica**: https://www.ulusofona.pt/documentos?cat=3\n",
    "- **Objetos Perdidos**: https://www.ulusofona.pt/en/lost-and-found\n",
    "- **Regulamentos**: https://www.ulusofona.pt/en/documents?cat=1\n",
    "- **Reshape**: https://secure.ensinolusofona.pt/reshape/\n",
    "- **Serviços**: https://www.ulusofona.pt/en/services\n",
    "- **Normas de Dissertações e Teses**: https://www.ulusofona.pt/media/normas-para-elaboracao-e-apresentacao-de-dissertacoes-e-teses.pdf\n",
    "\n",
    "### Internacional\n",
    "\n",
    "- **Estudantes Brasileiros**: https://www.ulusofona.pt/en/international-students/brazilian-students\n",
    "- **Estudantes Internacionais**: https://www.ulusofona.pt/en/international-students\n",
    "- **FILMEU - Universidade Europeia**: https://www.filmeu.eu/\n",
    "- **Mobilidade Estudantil**: https://www.ulusofona.pt/en/mobility\n",
    "\n",
    "### Estudantes\n",
    "\n",
    "- **Calendário de Defesas de Dissertações**: https://www.ulusofona.pt/en/theses\n",
    "- **Aplicativo Ensino Lusófona**: https://www.ulusofona.pt/en/services/mobile-app\n",
    "- **Cartão de Estudante**: https://www.ulusofona.pt/en/news/students-card\n",
    "- **Estágios**: https://eva.ulusofona.pt/\n",
    "- **Estudantes**: https://www.ulusofona.pt/en/student\n",
    "- **Necessidades Educativas Especiais**: https://www.ulusofona.pt/en/gaenee\n",
    "- **Portal de Emprego**: https://eva.ulusofona.pt/portal-de-emprego-universia/\n",
    "- **Orientador Estudantil**: https://www.ulusofona.pt/en/student-advisor\n",
    "- **Bolsas de Estudo**: https://www.ulusofona.pt/en/acao-social-escolar\n",
    "- **Vantagens e Benefícios**: https://www.ensinolusofona.pt/pt/vantagens\n",
    "\n",
    "### Comunidade\n",
    "\n",
    "- **Quartas na Lusófona**: https://www.ulusofona.pt/en/event/as-quartas-na-lusofona-23-24\n",
    "- **Cinema Fernando Lopes**: https://www.ulusofona.pt/cinema-fernando-lopes\n",
    "- **Construir Conhecimento**: https://www.ulusofona.pt/en/building-knowledge\n",
    "- **Jornadas de Abertura**: https://www.ulusofona.pt/en/open-days\n",
    "- **Escola Sénior**: https://escolasenior.ulusofona.pt/\n",
    "- **Escola de Verão**: https://escolaverao.ulusofona.pt/\n",
    "- **Hospital Veterinário - Marcação de Consultas**: https://www.ulusofona.pt/en/news/appointments-veterinary-hospital-\n",
    "- **Lusófona Talks**: https://www.ulusofona.pt/en/lusofona-talks\n",
    "- **Lusófona Verde**: https://www.ulusofona.pt/en/green-lusofona\n",
    "\n",
    "### Media e Eventos\n",
    "\n",
    "- **Crónicas**: https://www.ulusofona.pt/en/chronicles\n",
    "- **Lições**: https://www.ulusofona.pt/en/lessons\n",
    "- **Lusófona nos Media**: https://www.ulusofona.pt/en/lusofona-in-the-media\n",
    "- **Minha História - Testemunhos**: https://www.ulusofona.pt/en/testimonials\n",
    "- **Notícias**: https://www.ulusofona.pt/en/news\n",
    "- **Podcast - Direta Sem Café**: https://www.ulusofona.pt/en/news/direta-sem-cafe-podcast-lusofona\n",
    "\n",
    "```\n",
    "\n",
    "'''\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=GROQ_API_KEY, temperature=0.1)\n",
    "\n",
    "    def complete(self, prompt, context=None, system_prompt=SYSTEM_PROMPT):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        if context is not None:\n",
    "            messages.append(ChatMessage(role=\"user\", content=f\"Context: {context}\"))\n",
    "        \n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "    def chat(self, messages):\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Step 1: Initial processing\n",
    "    initial_prompt = f\"{content}\\n\\n{APPEND_PROMPT}\"\n",
    "    initial_response = llm.complete(initial_prompt)\n",
    "\n",
    "    # Step 2: Revision\n",
    "    revision_messages = [\n",
    "        ChatMessage(role=\"system\", content=SYSTEM_PROMPT),\n",
    "        ChatMessage(role=\"user\", content=initial_prompt),\n",
    "        ChatMessage(role=\"assistant\", content=initial_response),\n",
    "        ChatMessage(role=\"user\", content=REVISOR_PROMPT),\n",
    "    ]\n",
    "    final_response = llm.chat(revision_messages)\n",
    "\n",
    "    return final_response #.message.content\n",
    "\n",
    "def main():\n",
    "    llm = GroqLLM()\n",
    "    input_folder = 'output2'\n",
    "    output_folder = 'output_clean_formatted'\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.md'):  # Assuming the files are markdown\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            print(f\"Processing {filename}...\")\n",
    "            processed_content = process_file(input_path, llm)\n",
    "\n",
    "            # Save the processed content\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_content)\n",
    "\n",
    "            print(f\"Saved processed content to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='processing_log.txt', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "SYSTEM_PROMPT = '''\n",
    "Format each document precisely in Markdown, ensuring that absolutely all original content, language, and meaning are preserved in the output. Each message will handle one document at a time, and every single piece of content from the raw input must be included in the formatted output—regardless of whether the document is long or short.\n",
    "\n",
    "Do not modify or edit any information beyond the necessary formatting adjustments for clarity and consistency. The output must strictly adhere to these rules:\n",
    "\n",
    "- Avoid generic or redundant phrases describing processes (e.g., stating that something has been reviewed, formatted, or updated).\n",
    "- Eliminate formal summaries such as 'Here is the result' or 'The document now meets requirements.'\n",
    "- Focus solely on presenting the content directly, without any explanations of actions taken or process descriptions.\n",
    "- Ensure the final output is concise, clear, and follows markdown structure, avoiding any filler statements or unnecessary commentary.\n",
    "- The first line of the output must be exactly the same first line of raw document. Example: \"# Response for https://<example.pt>\"\n",
    "\n",
    "The output must be a complete representation of the raw document's content, formatted as required, with no omissions.\n",
    "'''\n",
    "\n",
    "#\"llama-guard-3-8b\"\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        def __init__(self):\n",
    "            self.models = [\n",
    "                {\"name\": \"llama-3.1-70b-versatile\", \"context_window\": 128000},  # 128k tokens, but max_tokens limited to 8k\n",
    "                {\"name\": \"llama-3.1-8b-instant\", \"context_window\": 128000},     # 128k tokens, but max_tokens limited to 8k\n",
    "                {\"name\": \"mixtral-8x7b-32768\", \"context_window\": 32768},\n",
    "                {\"name\": \"llama3-70b-8192\", \"context_window\": 8192},\n",
    "                {\"name\": \"llama3-8b-8192\", \"context_window\": 8192},\n",
    "                {\"name\": \"gemma2-9b-it\", \"context_window\": 8192},\n",
    "                {\"name\": \"gemma-7b-it\", \"context_window\": 8192},\n",
    "                {\"name\": \"llama-3.2-90b-vision-preview\", \"context_window\": 8192},  # 128k tokens, but temporarily limited to 8k in preview\n",
    "                {\"name\": \"llama-3.2-11b-vision-preview\", \"context_window\": 8192},  # 128k tokens, but temporarily limited to 8k in preview\n",
    "                {\"name\": \"llama3-groq-70b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "                {\"name\": \"llama3-groq-8b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "                {\"name\": \"llama-3.2-3b-preview\", \"context_window\": 8192},  # 128k tokens, but temporarily limited to 8k in preview\n",
    "                {\"name\": \"llama-3.2-1b-preview\", \"context_window\": 8192},  # 128k tokens, but temporarily limited to 8k in preview\n",
    "                {\"name\": \"llama-guard-3-8b\", \"context_window\": 8192},\n",
    "                {\"name\": \"llava-v1.5-7b-4096-preview\", \"context_window\": 4096},\n",
    "            ]\n",
    "            self.current_model_index = 0\n",
    "            self.llm = self.create_llm()\n",
    "            self.request_count = 0\n",
    "            self.last_request_time = 0\n",
    "\n",
    "    def create_llm(self):\n",
    "        return Groq(model=self.models[self.current_model_index][\"name\"], api_key=GROQ_API_KEY, temperature=0.1)\n",
    "    \n",
    "    def rotate_model(self):\n",
    "        self.current_model_index = (self.current_model_index + 1) % len(self.models)\n",
    "        self.llm = self.create_llm()\n",
    "        logging.info(f\"Switched to model: {self.models[self.current_model_index]}\")\n",
    "\n",
    "    def respect_rate_limit(self):\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_request_time < 60:  # 60 seconds window\n",
    "            if self.request_count >= 14400 / 1440:  # 14400 requests per day, divided by minutes in a day\n",
    "                sleep_time = 60 - (current_time - self.last_request_time)\n",
    "                logging.info(f\"Rate limit approached. Sleeping for {sleep_time:.2f} seconds\")\n",
    "                time.sleep(sleep_time)\n",
    "                self.request_count = 0\n",
    "                self.last_request_time = time.time()\n",
    "        else:\n",
    "            self.request_count = 0\n",
    "            self.last_request_time = current_time\n",
    "        self.request_count += 1\n",
    "\n",
    "    def chat(self, messages, max_retries=3, retry_delay=5):\n",
    "        original_model_index = self.current_model_index\n",
    "        for attempt in range(max_retries * len(self.models)):\n",
    "            try:\n",
    "                self.respect_rate_limit()\n",
    "                response = self.llm.chat(messages)\n",
    "                logging.info(f\"Successfully processed using model: {self.models[self.current_model_index]}\")\n",
    "                return response.message.content\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                logging.warning(f\"Error with model {self.models[self.current_model_index]}: {error_message}. Attempt {attempt + 1} of {max_retries * len(self.models)}\")\n",
    "                \n",
    "                if \"429\" in error_message:\n",
    "                    # Rate limit error\n",
    "                    retry_after = int(error_message.split(\"Please try again in \")[1].split(\"s.\")[0])\n",
    "                    logging.info(f\"Rate limit exceeded. Waiting for {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                elif \"400\" in error_message and \"maximum context length\" in error_message.lower():\n",
    "                    # Context length error\n",
    "                    logging.info(f\"Context length exceeded for model {self.models[self.current_model_index]}. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                elif \"404\" in error_message and \"model does not exist\" in error_message.lower():\n",
    "                    # Model not found error\n",
    "                    logging.info(f\"Model {self.models[self.current_model_index]} not found. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                else:\n",
    "                    # Other errors\n",
    "                    if attempt % max_retries == max_retries - 1:\n",
    "                        logging.info(f\"Rotating to next model after multiple failures.\")\n",
    "                        self.rotate_model()\n",
    "                    else:\n",
    "                        logging.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                        time.sleep(retry_delay)\n",
    "                \n",
    "                # If we've tried all models and are back to the original, break the loop\n",
    "                if self.current_model_index == original_model_index and attempt >= len(self.models):\n",
    "                    break\n",
    "\n",
    "        logging.error(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "        raise Exception(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_model_index][\"name\"]\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=SYSTEM_PROMPT),\n",
    "        ChatMessage(role=\"user\", content=content)\n",
    "    ]\n",
    "\n",
    "    current_model = llm.get_current_model()\n",
    "    logging.info(f\"Processing file {file_path} with model: {current_model}\")\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    \n",
    "    logging.info(f\"Finished processing {file_path} with model: {current_model}\")\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    llm = GroqLLM()\n",
    "    input_folder = 'out'\n",
    "    output_folder = 'output_clean_formatted'\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(input_folder) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files_to_process)} .md files in the input folder.\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            logging.info(f\"Skipping {filename} as it already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Starting to process {filename}...\")\n",
    "        try:\n",
    "            processed_content = process_file(input_path, llm)\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_content)\n",
    "\n",
    "            logging.info(f\"Saved processed content to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {filename}: {str(e)}\")\n",
    "            logging.exception(\"Exception details:\")\n",
    "\n",
    "    logging.info(\"Finished processing all files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m         logging\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException details:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 134\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     processed_content \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    137\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(processed_content)\n",
      "Cell \u001b[0;32mIn[36], line 107\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path, llm)\u001b[0m\n\u001b[1;32m    104\u001b[0m current_model \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mget_current_model()\n\u001b[1;32m    105\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[36], line 53\u001b[0m, in \u001b[0;36mGroqLLM.chat\u001b[0;34m(self, messages, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrespect_rate_limit()\n\u001b[0;32m---> 53\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed using model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_model()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/llms/openai_like/base.py:117\u001b[0m, in \u001b[0;36mOpenAILike.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     completion_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomplete(prompt, formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion_response_to_chat_response(completion_response)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:173\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    165\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    166\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     },\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    175\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    176\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    177\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m    178\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/llms/openai/base.py:353\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/llama_index/llms/openai/base.py:106\u001b[0m, in \u001b[0;36mllm_retry_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m retry \u001b[38;5;241m=\u001b[39m create_retry_decorator(\n\u001b[1;32m    100\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    101\u001b[0m     random_exponential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     max_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/tenacity/__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/rag_site_scan/.venv/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            {\"name\": \"llama-3.1-70b-versatile\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.1-8b-instant\", \"context_window\": 8192},\n",
    "            {\"name\": \"mixtral-8x7b-32768\", \"context_window\": 32768},\n",
    "            {\"name\": \"llama3-70b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-8b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma2-9b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma-7b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-90b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-11b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-70b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-8b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-3b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-1b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-guard-3-8b\", \"context_window\": 8192},\n",
    "            {\"name\": \"llava-v1.5-7b-4096-preview\", \"context_window\": 4096},\n",
    "        ]\n",
    "        self.current_model_index = 0\n",
    "        self.llm = None\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "        self.initialize_llm()\n",
    "\n",
    "    def initialize_llm(self):\n",
    "        self.llm = Groq(model=self.models[self.current_model_index][\"name\"], api_key=GROQ_API_KEY, temperature=0.1)\n",
    "\n",
    "    def rotate_model(self):\n",
    "        self.current_model_index = (self.current_model_index + 1) % len(self.models)\n",
    "        self.initialize_llm()\n",
    "        logging.info(f\"Switched to model: {self.get_current_model()}\")\n",
    "\n",
    "    def respect_rate_limit(self):\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_request_time < 60:  # 60 seconds window\n",
    "            if self.request_count >= 14400 / 1440:  # 14400 requests per day, divided by minutes in a day\n",
    "                sleep_time = 60 - (current_time - self.last_request_time)\n",
    "                logging.info(f\"Rate limit approached. Sleeping for {sleep_time:.2f} seconds\")\n",
    "                time.sleep(sleep_time)\n",
    "                self.request_count = 0\n",
    "                self.last_request_time = time.time()\n",
    "        else:\n",
    "            self.request_count = 0\n",
    "            self.last_request_time = current_time\n",
    "        self.request_count += 1\n",
    "\n",
    "    def chat(self, messages, max_retries=3, retry_delay=5):\n",
    "        original_model_index = self.current_model_index\n",
    "        for attempt in range(max_retries * len(self.models)):\n",
    "            try:\n",
    "                self.respect_rate_limit()\n",
    "                response = self.llm.chat(messages)\n",
    "                logging.info(f\"Successfully processed using model: {self.get_current_model()}\")\n",
    "                return response.message.content\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                logging.warning(f\"Error with model {self.get_current_model()}: {error_message}. Attempt {attempt + 1} of {max_retries * len(self.models)}\")\n",
    "                \n",
    "                if \"429\" in error_message:\n",
    "                    # Rate limit error\n",
    "                    retry_after = int(error_message.split(\"Please try again in \")[1].split(\"s.\")[0])\n",
    "                    logging.info(f\"Rate limit exceeded. Waiting for {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                elif \"400\" in error_message and \"maximum context length\" in error_message.lower():\n",
    "                    # Context length error\n",
    "                    logging.info(f\"Context length exceeded for model {self.get_current_model()}. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                elif \"404\" in error_message and \"model does not exist\" in error_message.lower():\n",
    "                    # Model not found error\n",
    "                    logging.info(f\"Model {self.get_current_model()} not found. Rotating to next model.\")\n",
    "                    self.rotate_model()\n",
    "                else:\n",
    "                    # Other errors\n",
    "                    if attempt % max_retries == max_retries - 1:\n",
    "                        logging.info(f\"Rotating to next model after multiple failures.\")\n",
    "                        self.rotate_model()\n",
    "                    else:\n",
    "                        logging.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                        time.sleep(retry_delay)\n",
    "                \n",
    "                # If we've tried all models and are back to the original, break the loop\n",
    "                if self.current_model_index == original_model_index and attempt >= len(self.models):\n",
    "                    break\n",
    "\n",
    "        logging.error(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "        raise Exception(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_model_index][\"name\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=SYSTEM_PROMPT),\n",
    "        ChatMessage(role=\"user\", content=content)\n",
    "    ]\n",
    "\n",
    "    current_model = llm.get_current_model()\n",
    "    logging.info(f\"Processing file {file_path} with model: {current_model}\")\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    \n",
    "    logging.info(f\"Finished processing {file_path} with model: {current_model}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        llm = GroqLLM()\n",
    "        input_folder = 'out'\n",
    "        output_folder = 'output_clean_formatted'\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        files_to_process = [f for f in os.listdir(input_folder) if f.endswith('.md')]\n",
    "        logging.info(f\"Found {len(files_to_process)} .md files in the input folder.\")\n",
    "\n",
    "        for filename in files_to_process:\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                logging.info(f\"Skipping {filename} as it already exists in the output folder.\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Starting to process {filename}...\")\n",
    "            try:\n",
    "                processed_content = process_file(input_path, llm)\n",
    "\n",
    "                with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(processed_content)\n",
    "\n",
    "                logging.info(f\"Saved processed content to {output_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filename}: {str(e)}\")\n",
    "                logging.exception(\"Exception details:\")\n",
    "\n",
    "        logging.info(\"Finished processing all files.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            {\"name\": \"llama-3.1-70b-versatile\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.1-8b-instant\", \"context_window\": 8192},\n",
    "            {\"name\": \"mixtral-8x7b-32768\", \"context_window\": 32768},\n",
    "            {\"name\": \"llama3-70b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-8b-8192\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma2-9b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"gemma-7b-it\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-90b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-11b-vision-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-70b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama3-groq-8b-8192-tool-use-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-3b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-3.2-1b-preview\", \"context_window\": 8192},\n",
    "            {\"name\": \"llama-guard-3-8b\", \"context_window\": 8192},\n",
    "            {\"name\": \"llava-v1.5-7b-4096-preview\", \"context_window\": 4096},\n",
    "        ]\n",
    "        self.current_model_index = 0\n",
    "        self.last_request_time = 0\n",
    "        self.requests_in_window = 0\n",
    "        self.rate_limit = 500000  # tokens per minute\n",
    "        self.window_size = 60  # 1 minute window\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_model_index]\n",
    "\n",
    "    def rotate_model(self):\n",
    "        self.current_model_index = (self.current_model_index + 1) % len(self.models)\n",
    "        logging.info(f\"Rotating to model: {self.get_current_model()['name']}\")\n",
    "\n",
    "    def wait_for_rate_limit(self, tokens_requested):\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "\n",
    "        if time_since_last_request >= self.window_size:\n",
    "            self.requests_in_window = 0\n",
    "        else:\n",
    "            self.requests_in_window -= max(0, (self.requests_in_window * time_since_last_request / self.window_size))\n",
    "\n",
    "        if self.requests_in_window + tokens_requested > self.rate_limit:\n",
    "            wait_time = (self.requests_in_window + tokens_requested - self.rate_limit) / (self.rate_limit / self.window_size)\n",
    "            logging.info(f\"Rate limit approached. Waiting for {wait_time:.2f} seconds\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        self.requests_in_window += tokens_requested\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def chat(self, messages, max_retries=3):\n",
    "        for attempt in range(max_retries * len(self.models)):\n",
    "            current_model = self.get_current_model()\n",
    "            llm = Groq(model=current_model[\"name\"])\n",
    "\n",
    "            try:\n",
    "                # Estimate token count (this is a rough estimate, you may want to use a proper tokenizer)\n",
    "                tokens_requested = sum(len(msg.content.split()) for msg in messages) * 1.3\n",
    "\n",
    "                self.wait_for_rate_limit(tokens_requested)\n",
    "\n",
    "                response = llm.chat(messages)\n",
    "                return response.message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                logging.warning(f\"Error with model {current_model['name']}: {error_message}. Attempt {attempt + 1} of {max_retries * len(self.models)}\")\n",
    "\n",
    "                if \"429\" in error_message:\n",
    "                    retry_after = 60  # Default to 60 seconds if we can't parse the time\n",
    "                    try:\n",
    "                        retry_after = int(float(error_message.split(\"Please try again in \")[1].split(\"s\")[0]))\n",
    "                    except:\n",
    "                        pass\n",
    "                    logging.info(f\"Rate limit exceeded. Waiting for {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                    self.rotate_model()\n",
    "                elif attempt % max_retries == max_retries - 1:\n",
    "                    self.rotate_model()\n",
    "                else:\n",
    "                    time.sleep(5)  # Wait for 5 seconds before retrying with the same model\n",
    "\n",
    "        logging.error(\"Failed to get a response after trying all models and multiple retries.\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path, llm):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are a helpful assistant that cleans and formats text.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"Please clean and format the following text, removing any HTML tags, fixing formatting issues, and ensuring it's well-structured:\\n\\n{content}\")\n",
    "        ]\n",
    "\n",
    "        processed_content = llm.chat(messages)\n",
    "        return processed_content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {file_path}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def main():\n",
    "    llm = GroqLLM()\n",
    "    input_folder = 'out'\n",
    "    output_folder = 'output_clean_formatted'\n",
    "    failed_folder = 'failed_processing'\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(failed_folder, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(input_folder) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files_to_process)} .md files in the input folder.\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            logging.info(f\"Skipping {filename} as it already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Starting to process {filename}...\")\n",
    "        processed_content = process_file(input_path, llm)\n",
    "\n",
    "        if processed_content:\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_content)\n",
    "            logging.info(f\"Saved processed content to {output_path}\")\n",
    "        else:\n",
    "            failed_path = os.path.join(failed_folder, filename)\n",
    "            os.rename(input_path, failed_path)\n",
    "            logging.warning(f\"Moved {filename} to {failed_folder} due to processing failure.\")\n",
    "\n",
    "    logging.info(\"Finished processing all files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script clean output_clean_formatted folder of files create from a llama-2 call gone wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: output_clean_formatted/en_teachers_julio-do-carmo-barata-3048.md\n",
      "Deleted file: output_clean_formatted/docentes_angelina-maria-da-costa-santos-2434.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_comunicacao-e-jornalismo_ULHT449-8403.md\n",
      "Deleted file: output_clean_formatted/porto_licenciaturas_ciencias-da-comunicacao_ULP451-26060.md\n",
      "Deleted file: output_clean_formatted/docentes_maria-de-lurdes-coentro-vargas-6105.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_cinema-e-artes-dos-media_ULHT160-24626.md\n",
      "Deleted file: output_clean_formatted/en_teachers_jose-gregorio-viegas-bras-543.md\n",
      "Deleted file: output_clean_formatted/docentes_catarina-de-moura-coelho-6840.md\n",
      "Deleted file: output_clean_formatted/docentes_flavio-jose-serrano-roques-2101.md\n",
      "Deleted file: output_clean_formatted/en_news_sustainability-movement-.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_design-e-producao-de-moda_ULP6805-25318.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_applied-communication_ULP1652-24196.md\n",
      "Deleted file: output_clean_formatted/en_teachers_jorge-manuel-horta-trigo-mira-1444.md\n",
      "Deleted file: output_clean_formatted/en_porto_bachelor_psychology_ULP608-22178.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_design-e-producao-de-moda_ULP6805-25327.md\n",
      "Deleted file: output_clean_formatted/en_event_lusofonativa-apresentacao-resultados.md\n",
      "Deleted file: output_clean_formatted/en_teachers_jose-manuel-de-figueiredo-gomes-pinto-1837.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_engenharia-e-gestao-industrial_docentes.md\n",
      "Deleted file: output_clean_formatted/porto_mestrados-integrados_arquitetura_ULP286-16154.md\n",
      "Deleted file: output_clean_formatted/evento_fl-talk-moda-sustentabilidade.md\n",
      "Deleted file: output_clean_formatted/docentes_daniel-filipe-seica-neves-cruzeiro-254.md\n",
      "Deleted file: output_clean_formatted/porto_mestrados-integrados_arquitetura_ULP286-25532.md\n",
      "Deleted file: output_clean_formatted/porto_licenciaturas_turismo-e-gestao-de-empresas-turisticas_ULP623-5017.md\n",
      "Deleted file: output_clean_formatted/en_porto_integrated-masters_architecture_ULP286-14480.md\n",
      "Deleted file: output_clean_formatted/en_teachers_ana-alexandra-da-conceicao-mirco-fernandes-4501.md\n",
      "Deleted file: output_clean_formatted/noisi_academica-lusofona.md\n",
      "Deleted file: output_clean_formatted/lisboa_doutoramentos_psicologia-clinica-orientacao-cognitivo-comportamental_ULHT6228-22749.md\n",
      "Deleted file: output_clean_formatted/en_teachers_joao-filipe-gulherme-goncalves-7687.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_commercial-management_ULP6623-2536.md\n",
      "Deleted file: output_clean_formatted/porto_mestrados-integrados_arquitetura_ULP286-25530.md\n",
      "Deleted file: output_clean_formatted/lisboa_doutoramentos_ciencias-da-comunicacao_docentes.md\n",
      "Deleted file: output_clean_formatted/en_teachers_nuno-rafael-da-silva-peres-7241.md\n",
      "Deleted file: output_clean_formatted/en_porto_bachelor_sustainable-fashion-design_ULP6805-25311.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_performing-arts-actor-training_ULP1977-10131.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_cinema-and-media-arts_ULHT160-24622.md\n",
      "Deleted file: output_clean_formatted/en_teachers_andre-rui-nunes-bernardes-da-cunha-graca-6684.md\n",
      "Deleted file: output_clean_formatted/en_teachers_ligia-cristina-ferreira-roque-7862.md\n",
      "Deleted file: output_clean_formatted/en_porto_phd_clinical-psychology-cognitive-behavioral-approach_ULP6229-22747.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_education-sciences-social-education_ULHT75-336.md\n",
      "Deleted file: output_clean_formatted/docentes_ines-castelo-branco-de-oliveira-santos-1944.md\n",
      "Deleted file: output_clean_formatted/docentes_telma-filipa-antunes-lopes-lourenco-2189.md\n",
      "Deleted file: output_clean_formatted/en_teachers_andreia-sofia-feliciano-nunes-6087.md\n",
      "Deleted file: output_clean_formatted/en_teachers_rafael-manuel-patrao-tavares-antunes-martins-5337.md\n",
      "Deleted file: output_clean_formatted/docentes_cristiane-da-silva-ferreira-nunes-3715.md\n",
      "Deleted file: output_clean_formatted/en_teachers_francisco-fernandes-castro-rego-7577.md\n",
      "Deleted file: output_clean_formatted/docentes_jorge-manuel-paixao-da-costa-504.md\n",
      "Deleted file: output_clean_formatted/en_phd.md\n",
      "Deleted file: output_clean_formatted/en_teachers_alexandre-joaquim-da-silva-marques-6796.md\n",
      "Deleted file: output_clean_formatted/docentes_mario-ricardo-silveiro-de-barros-8070.md\n",
      "Deleted file: output_clean_formatted/lisboa_erasmus-mundus_ciberespaco-comportamento-e-terapia-digital_ULHT6802-25354.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_integrated-masters_architecture_ULP286-14463.md\n",
      "Deleted file: output_clean_formatted/en_teachers_miguel-alberto-adao-pinto-7264.md\n",
      "Deleted file: output_clean_formatted/lisboa_erasmus-mundus_doc-nomads-documentary-film-direction_ULHT1529-14185.md\n",
      "Deleted file: output_clean_formatted/porto_licenciaturas_computacao-e-matematica-aplicada_ULP6639-24447.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_cinema-e-artes-dos-media_ULHT160-24615.md\n",
      "Deleted file: output_clean_formatted/docentes_francisco-manuel-pereira-fialho-camacho-2611.md\n",
      "Deleted file: output_clean_formatted/en_teachers_samuel-gil-da-costa-marques-6483.md\n",
      "Deleted file: output_clean_formatted/docentes_simao-pedro-ferreira-costa-7972.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_communication-science_ULHT24-3321.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_management_ULP292-21711.md\n",
      "Deleted file: output_clean_formatted/lisboa_doutoramentos_educacao-fisica-e-desporto_ULHT1108-12597.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_artes-da-animacao_ULHT2165-25552.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_phd_informatics_teachers.md\n",
      "Deleted file: output_clean_formatted/docentes_paulo-antonio-martins-barreiros-6191.md\n",
      "Deleted file: output_clean_formatted/porto_licenciaturas_computacao-e-matematica-aplicada_ULP6639-7337.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_integrated-masters_veterinary-medicine_ULHT478-54.md\n",
      "Deleted file: output_clean_formatted/docentes_rute-maria-da-silva-proenca-muchacho-1039.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_management_ULP292-6587.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_psicologia_ULP608-13887.md\n",
      "Deleted file: output_clean_formatted/porto_doutoramentos_direito_ULP6230-22760.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_comunicacao-audiovisual-e-multimedia_ULP728-4444.md\n",
      "Deleted file: output_clean_formatted/en_teachers_antonio-pedro-raposo-marques-vidal-161.md\n",
      "Deleted file: output_clean_formatted/en_teachers_miguel-nuno-vieira-de-carvalho-dabreu-varela-900638.md\n",
      "Deleted file: output_clean_formatted/docentes_bruno-filipe-de-sousa-albuquerque-6413.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_ensino-de-educacao-fisica-nos-ensinos-basico-e-secundario_docentes.md\n",
      "Deleted file: output_clean_formatted/docentes_luis-manuel-monteiro-alves-4812.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_phd_education_ULHT482-14317.md\n",
      "Deleted file: output_clean_formatted/en_validation-of-issued-documents.md\n",
      "Deleted file: output_clean_formatted/docentes_joao-paulo-costa-braz-6134.md\n",
      "Deleted file: output_clean_formatted/porto_mestrados_direito_docentes.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_patrimonio-cinematografico-e-audiovisual_docentes.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_desenvolvimento-e-gestao-de-destinos-turisticos_ULHT6447-7334.md\n",
      "Deleted file: output_clean_formatted/docentes_mario-antonio-lage-alves-marques-5500.md\n",
      "Deleted file: output_clean_formatted/lisboa_doutoramentos_direito_ULP6230-15208.md\n",
      "Deleted file: output_clean_formatted/porto_doutoramentos_arquitetura_docentes.md\n",
      "Deleted file: output_clean_formatted/docentes_andre-duarte-martins-da-silva-58325.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_cinema-e-artes-dos-media_ULHT160-24606.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_engenharia-eletrotecnica-de-sistemas-de-energia_ULP732-22369.md\n",
      "Deleted file: output_clean_formatted/en_porto_integrated-masters_architecture_ULP286-25530.md\n",
      "Deleted file: output_clean_formatted/faqs_docentes_secretaria-virtual_quanto-tempo-tenho-para-lancar-o-sumario.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_communication-science_ULHT24-1945.md\n",
      "Deleted file: output_clean_formatted/en_teachers_rui-manuel-de-figueiredo-marcos-1029.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_educacao-fisica-e-desporto_ULHT495-3-17100.md\n",
      "Deleted file: output_clean_formatted/docentes_henrique-jose-de-ancede-aroso-barros-basto-6151.md\n",
      "Deleted file: output_clean_formatted/en_teachers_elvis-manuel-de-jesus-veiguinha-4045.md\n",
      "Deleted file: output_clean_formatted/en_porto_bachelor_environmental-engineering_ULP287-1919.md\n",
      "Deleted file: output_clean_formatted/en_teachers_joao-carlos-gil-da-silva-ribeiro-4869.md\n",
      "Deleted file: output_clean_formatted/lisboa_doutoramentos_educacao-fisica-e-desporto_ULHT1108-12596.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_gestao-financeira_ULHT6982-21819.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_physical-education-and-sport_ULHT495-8670.md\n",
      "Deleted file: output_clean_formatted/en_porto_bachelor_psychology_ULP608-23777.md\n",
      "Deleted file: output_clean_formatted/en_podcasts_ep2-making-difference-people-promoting-well-being-helia-bracons.md\n",
      "Deleted file: output_clean_formatted/lisboa_licenciaturas_artes-dramaticas-formacao-de-atores_ULP1977-15448.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_data-science_teachers.md\n",
      "Deleted file: output_clean_formatted/en_teachers_mara-mileu-rodrigues-7185.md\n",
      "Deleted file: output_clean_formatted/en_teachers_idalino-andre-rodrigues-nascimento-magrinho-3798.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_training_cirurgia-ortopedica-e-neurocirurgia-em-animais-de-companhia.md\n",
      "Deleted file: output_clean_formatted/en_event_reasearch-treatment-perpetrator-programs.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_exercicio-e-bem-estar_ULHT792-10901.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_neuropsicologia-aplicada_docentes.md\n",
      "Deleted file: output_clean_formatted/docentes_carla-sofia-pinho-santos-7122.md\n",
      "Deleted file: output_clean_formatted/lisboa_mestrados_engenharia-do-ambiente_ULHT503-13369.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_masters_animation-arts_ULHT2165-25552.md\n",
      "Deleted file: output_clean_formatted/en_lisboa_bachelor_physical-education-and-sport_ULP1267-1919.md\n",
      "Deleted file: output_clean_formatted/en_teachers_goncalo-laidley-melo-galvao-teles-4434.md\n",
      "Deleted file: output_clean_formatted/en_teachers_nuno-filipe-inacio-vaz-matias-6852.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def delete_matching_files(log_file_path, output_folder):\n",
    "    \"\"\"\n",
    "    Reads through a log file, finds matching files and deletes them from the output folder.\n",
    "\n",
    "    Args:\n",
    "    - log_file_path (str): Path to the log file.\n",
    "    - output_folder (str): Path to the output folder.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the log file\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Use regular expression to find the file name\n",
    "                match = re.search(r'Finished processing out/(.+?).md with model: llama-2-70b-chat', line)\n",
    "                if match:\n",
    "                    file_name = match.group(1)\n",
    "                    file_path = os.path.join(output_folder, file_name + '.md')\n",
    "                    if os.path.exists(file_path):\n",
    "                        # Delete the file\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted file: {file_path}\")\n",
    "                    else:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Log file not found: {log_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "log_file_path = 'processing_log.txt'\n",
    "output_folder = 'output_clean_formatted'\n",
    "delete_matching_files(log_file_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: python-dotenv 1.0.1\n",
      "Uninstalling python-dotenv-1.0.1:\n",
      "  Successfully uninstalled python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall python-dotenv -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
